{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHxTQNe7AS0F",
        "outputId": "2b7e5281-f053-44a5-a22f-2b915c829d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#----------------------\n",
        "# Mount Drive to Colab\n",
        "#----------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwmtyR0r-fV0"
      },
      "source": [
        "## Text Retrieval and Search Engines - Homework 1 ##\n",
        "Micaela Singer <br>\n",
        "Tomer Hadad \n",
        "### a. Inverted Index ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKKRWL_b--z_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from nltk.stem import *\n",
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rm692HPHQyd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, files_path):\n",
        "        self.files_path = files_path\n",
        "\n",
        "        # internal-external document id maps\n",
        "        self.docno2docid = {}\n",
        "        self.docid2docno = {}\n",
        "\n",
        "        self.inverted_index = defaultdict(list)\n",
        "\n",
        "        # tokens to ignore (dont add to the inverted index dictionary)\n",
        "        self.blacklist = ['<DOC>', '<TEXT>', '<DOCNO>', '</DOC>', '</TEXT>', '</DOCNO>']\n",
        "\n",
        "    def create_index(self):\n",
        "        doc_id = 0\n",
        "        token_docno = False\n",
        "\n",
        "        # For every file in the dataset\n",
        "        for filename in os.listdir(self.files_path):\n",
        "            with open(os.path.join(self.files_path, filename), 'r') as file:\n",
        "                content = file.read()\n",
        "\n",
        "                # Split the file into tokens by \\n and whitespace\n",
        "                tokens = content.split()\n",
        "\n",
        "                # For every token in the file\n",
        "                for i, token in enumerate(tokens):\n",
        "\n",
        "                    # If it is the start of a new document\n",
        "                    if token == '<DOCNO>':\n",
        "                        doc_id += 1\n",
        "\n",
        "                        # Fetch the actual document id\n",
        "                        curr_doc_no = tokens[i+1]\n",
        "\n",
        "                        # Add the document internal and external ids to the map\n",
        "                        self.docno2docid[curr_doc_no] = doc_id\n",
        "                        self.docid2docno[doc_id] = curr_doc_no\n",
        "\n",
        "                        # Mark the next token to ignore\n",
        "                        token_docno = True\n",
        "\n",
        "                        # Print progress\n",
        "                        if doc_id % 10000 == 0:\n",
        "                            print(f'reached {doc_id} documents')\n",
        "                    \n",
        "                    # Remove the mark\n",
        "                    if token == '</DOCNO>':\n",
        "                        token_docno = False\n",
        "                    \n",
        "                    # If the token is not marked to ignore, and not in the blacklist, and the document id is not yet saved\n",
        "                    if not token_docno and token not in self.blacklist and (len(self.inverted_index[token]) == 0 or self.inverted_index[token][-1] < doc_id):\n",
        "\n",
        "                        # Add the document id at the end of the token's list\n",
        "                        self.inverted_index[token].append(doc_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1vTEXAuK-5C",
        "outputId": "df51443f-ef5a-4e35-8fc7-f9bd5bcf640f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reached 10000 documents\n",
            "reached 20000 documents\n",
            "reached 30000 documents\n",
            "reached 40000 documents\n",
            "reached 50000 documents\n",
            "reached 60000 documents\n",
            "reached 70000 documents\n",
            "reached 80000 documents\n",
            "reached 90000 documents\n",
            "reached 100000 documents\n",
            "reached 110000 documents\n",
            "reached 120000 documents\n",
            "reached 130000 documents\n",
            "reached 140000 documents\n",
            "reached 150000 documents\n",
            "reached 160000 documents\n",
            "reached 170000 documents\n",
            "reached 180000 documents\n",
            "reached 190000 documents\n",
            "reached 200000 documents\n",
            "reached 210000 documents\n",
            "reached 220000 documents\n",
            "reached 230000 documents\n",
            "reached 240000 documents\n"
          ]
        }
      ],
      "source": [
        "# Create the index from the dataset\n",
        "inverted_index = InvertedIndex(r\"/content/drive/MyDrive/Text Retrieval and Search Engines/Ex1\")\n",
        "inverted_index.create_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmG3frY5Qxu9"
      },
      "outputs": [],
      "source": [
        "# Classes for the query tree:\n",
        "\n",
        "# Base class\n",
        "class Node:\n",
        "    def __init__(self, inverted_index, value=None, children=None, is_include=True):\n",
        "        self.inverted_index = inverted_index\n",
        "        self.value = value\n",
        "        self.children = children\n",
        "        self.is_include = is_include\n",
        "\n",
        "    # Evaluate should be called once we would line to get the list of document ids of the query represented by the tree\n",
        "    def evaluate(self):\n",
        "        pass\n",
        "\n",
        "# Word - represents a word in the query\n",
        "class WORD(Node):\n",
        "    def __init__(self, inverted_index, value, is_include=True):\n",
        "        super().__init__(inverted_index, value, None, is_include)\n",
        "    \n",
        "    def evaluate(self):\n",
        "        return (self.is_include, self.inverted_index[self.value])\n",
        "\n",
        "# Or - represents an OR operator in the query\n",
        "class OR(Node):\n",
        "    def __init__(self, inverted_index, children):\n",
        "        super().__init__(inverted_index, None, children, True)\n",
        "\n",
        "    def evaluate(self):\n",
        "        # Evaluate the children nodes to get thier lists of document ids, and wether the list should be included or excluded\n",
        "        eval_children = []\n",
        "        for child in self.children:\n",
        "            eval_children.append(child.evaluate())\n",
        "\n",
        "        n = len(eval_children)\n",
        "        list_done = [False] * n\n",
        "        pointers = [0] * n\n",
        "        result = []\n",
        "        done_lists = 0\n",
        "        \n",
        "        # While some lists still contain unexplored values\n",
        "        while done_lists < n:\n",
        "            min_value = float('inf')\n",
        "\n",
        "            # Find the minimum document id between all lists and the current positions represented in 'pointers'\n",
        "            for i in range(n):\n",
        "                if not list_done[i] and eval_children[i][1][pointers[i]] < min_value:\n",
        "                    min_value = eval_children[i][1][pointers[i]]\n",
        "\n",
        "            # Add the document id to the result\n",
        "            result.append(min_value)\n",
        "\n",
        "            # For every document id list\n",
        "            for i in range(n):\n",
        "\n",
        "                # If we haven't reached the end of the list, and the current value is equal to the minimal value (the one added to the result)\n",
        "                if not list_done[i] and eval_children[i][1][pointers[i]] == min_value:\n",
        "\n",
        "                    # Move to the next element\n",
        "                    pointers[i] += 1\n",
        "\n",
        "                    # If we reached the end of the list, mark it as done\n",
        "                    if pointers[i] >= len(eval_children[i][1]):\n",
        "                        list_done[i] = True\n",
        "                        done_lists += 1\n",
        "            \n",
        "        return (self.is_include, result)\n",
        "\n",
        "# And - represents an AND operator in the query\n",
        "class AND(Node):\n",
        "    def __init__(self, inverted_index, children):\n",
        "        super().__init__(inverted_index, None, children, True)\n",
        "\n",
        "    def evaluate(self):\n",
        "        # Evaluate the children nodes to get thier lists of document ids, and wether the list should be included or excluded\n",
        "        eval_children = []\n",
        "        for child in self.children:\n",
        "            eval_children.append(child.evaluate())\n",
        "\n",
        "        n = len(eval_children)\n",
        "        pointers = [0] * n\n",
        "        result = []\n",
        "        list_done = [False] * n\n",
        "        done_lists = 0\n",
        "        \n",
        "        # While some lists still contain unexplored values\n",
        "        while done_lists < n:\n",
        "            # Get the minimum document id of the first non-excluded list\n",
        "            for i in range(n):\n",
        "                if pointers[i] < len(eval_children[i][1]) and eval_children[i][0]:\n",
        "                    min_value = eval_children[i][1][pointers[i]]\n",
        "                    found_min_value_list_index = i\n",
        "                    break\n",
        "\n",
        "            # For every list of document ids\n",
        "            for i in range(n):\n",
        "\n",
        "                # Skip all document ids that are smaller than ours (min_value)\n",
        "                while pointers[i] < len(eval_children[i][1]) and eval_children[i][1][pointers[i]] < min_value:\n",
        "                    pointers[i] += 1\n",
        "                \n",
        "                # If we reached the end of the list, mark it as done\n",
        "                if pointers[i] >= len(eval_children[i][1]):\n",
        "                    list_done[i] = True\n",
        "                    done_lists += 1\n",
        "                \n",
        "                # If the min_value fails (reached the end of an non-excluded list OR found an included list where the current value is different OR found an excluded list where the value is the same) break, and do not add the document id to the result\n",
        "                if (pointers[i] == len(eval_children[i][1]) and eval_children[i][0]) or \\\n",
        "                   (pointers[i] < len(eval_children[i][1]) and eval_children[i][0] and eval_children[i][1][pointers[i]] != min_value) or \\\n",
        "                   (pointers[i] < len(eval_children[i][1]) and not eval_children[i][0] and eval_children[i][1][pointers[i]] == min_value):\n",
        "                    break\n",
        "            else:\n",
        "                # If the for loop finished without breaking, add the document id to the result\n",
        "                result.append(min_value)\n",
        "            pointers[found_min_value_list_index] += 1\n",
        "        \n",
        "        return (self.is_include, result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BooleanRetrieval:\n",
        "    def __init__(self, inverted_index, boolean_query_file_path):\n",
        "        self.inverted_index = inverted_index\n",
        "        self.boolean_query_file_path = boolean_query_file_path\n",
        "    \n",
        "    def generate_query_tree(self, query):\n",
        "        operators = ['NOT', 'AND', 'OR']\n",
        "        stack = []\n",
        "        tokens = query.split()\n",
        "        \n",
        "        # For every token in the query\n",
        "        for i, token in enumerate(tokens):\n",
        "\n",
        "            # If it is not an operator (its a word)\n",
        "            if token not in operators:\n",
        "                \n",
        "                # Check if it should be excluded or included\n",
        "                is_not = i+1 < len(tokens) and tokens[i+1] == 'NOT'\n",
        "\n",
        "                # Add the word to the stack\n",
        "                stack.append(WORD(self.inverted_index.inverted_index, token, not is_not))\n",
        "\n",
        "            # If it is a AND operator\n",
        "            if token == 'AND':\n",
        "\n",
        "                # Empty the stack into the AND operator, and create a new stack with the new operator as the only value inside\n",
        "                op = AND(self.inverted_index.inverted_index, stack)\n",
        "                stack = [op]\n",
        "\n",
        "            # If it is a OR operator\n",
        "            if token == 'OR':\n",
        "\n",
        "                # Empty the stack into the OR operator, and create a new stack with the new operator as the only value inside\n",
        "                op = OR(self.inverted_index.inverted_index, stack)\n",
        "                stack = [op]\n",
        "        \n",
        "        # If the stack has more than one node inside, put all leftover nodes into an AND operator and return\n",
        "        if len(stack) > 1:\n",
        "            return AND(self.inverted_index.inverted_index, stack)\n",
        "        return stack[0]\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        # Read the queries file and create an output file\n",
        "        with open(self.boolean_query_file_path, 'r') as file:\n",
        "            with open('Part_2.txt', 'w') as output_file:\n",
        "                lines = file.readlines()\n",
        "                \n",
        "                # For every query\n",
        "                for i, line in enumerate(lines):\n",
        "                    print(f'Line: {i+1}')\n",
        "\n",
        "                    # Generate a query tree\n",
        "                    query_tree = self.generate_query_tree(line)\n",
        "\n",
        "                    # Evaluate the query tree\n",
        "                    _, result = query_tree.evaluate()\n",
        "\n",
        "                    # Write the external document ids into the output file\n",
        "                    for docid in result:\n",
        "                        output_file.write(self.inverted_index.docid2docno[docid] + ' ')\n",
        "                    output_file.write('\\n')"
      ],
      "metadata": {
        "id": "2_GKt-dxRjYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boolean_retrieval = BooleanRetrieval(inverted_index, r\"/content/drive/MyDrive/Text Retrieval and Search Engines/BooleanQueries.txt\")\n",
        "boolean_retrieval.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJsFoayBmHu0",
        "outputId": "7591d1f7-0c83-4798-fb71-0b9b956e3582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line: 1\n",
            "Line: 2\n",
            "Line: 3\n",
            "Line: 4\n",
            "Line: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort the keys by the length of their corresponding lists\n",
        "sorted_keys = sorted(inverted_index.inverted_index, key=lambda k: len(inverted_index.inverted_index[k]), reverse=True)\n",
        "\n",
        "# return the top 10 keys\n",
        "top_10_keys = sorted_keys[:10]\n",
        "bottom_10_keys = sorted_keys[-10:]\n",
        "\n",
        "print(top_10_keys)\n",
        "print(bottom_10_keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lDzXWIs4oEZ",
        "outputId": "a778d363-67a5-4ebf-c198-63d6619983ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'of', 'in', 'a', 'and', 'to', 'for', 'said', 'on', 'that']\n",
            "['30739', '26494', 'decertifications', '230p', '930a', 'stockcollar', 'mexpetro', '535p', 'almtriv', '118183']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8ipNWqo3nMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
